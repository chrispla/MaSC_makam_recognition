{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makam pitch sequence classification with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pitch files\n",
    "\n",
    "Before proceeding, the pitch files on the CompMusic Dunya makam corpus need to be converted in the quantized pitch series encoding constructed as described in the pseudocode below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for (all lines):\n",
    "--->delete value if 0\n",
    "--->quantize value to scale (e.g. 53ET) // pad start of pitch value with zeros appropriately so that the length of the pitch value is 5 e.g. 130 -> 00130\n",
    "    \n",
    "for (all lines):\n",
    "--->merge all neighboring values to one, and keep track of the number of consecutive occurrences k e.g. 1300\\n 1200\\n 1200\\n 1200\\n 760\\n 760\\n will become 1300,1\\n 1200,3\\n 760,2\\n\n",
    "    \n",
    "find kmin and kmax\t\n",
    "\n",
    "for (all lines):\n",
    "--->if k < (kmin + 0.25*(kmax-kmin)): replace with 1\n",
    "--->if (kmin + 0.25*(kmax-kmin)) < k < (kmin + 0.75*(kmax-kmin)): replace with 2\n",
    "--->If k > (kmin + 0.75*(kmax-kmin)): replace with 3\n",
    "\n",
    "//We now have a quantized pitch series encoding with a line format of pitch,significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library importing for file reading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-67189f5b9c86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmakam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmakams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./otmm_makam_recognition_dataset/data/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmakam\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmakam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "#quantized pitch file directory\n",
    "q_read_dir = \"./qdata/\" \n",
    "ofq_read_dir = \"./octfold_qdata/\" \n",
    "\n",
    "#Makam list for more efficient file searching during label retrieval\n",
    "makams = [\"Acemasiran\", \"Acemkurdi\", \"Bestenigar\", \"Beyati\", \"Hicaz\", \"Hicazkar\", \"Huseyni\", \"Huzzam\", \"Karcigar\", \"Kurdilihicazkar\", \"Mahur\", \"Muhayyer\", \"Neva\", \"Nihavent\", \"Rast\", \"Saba\", \"Segah\", \"Sultaniyegah\", \"Suzinak\", \"Ussak\"]\n",
    "\n",
    "X = [] #array holding pitch series split per line\n",
    "y = [] #holds makam labels\n",
    "max_length = 0\n",
    "for root, dirs, files in os.walk(ofq_read_dir):\n",
    "    for name in files:\n",
    "        if '.pitch' in name:\n",
    "            #retrieve label from parent of original path\n",
    "            for makam in makams:\n",
    "                if (os.path.isfile(\"./otmm_makam_recognition_dataset/data/\" + makam + \"/\" + name) == True):\n",
    "                    y.append(makam)\n",
    "                    break\n",
    "            with open(os.path.join(root, name)) as f:\n",
    "                content = f.readlines()\n",
    "                content_length = len(content)\n",
    "                if (content_length > max_length):\n",
    "                    max_length = content_length\n",
    "                X.append(content)\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Pading input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence padding\n",
    "X = sequence.pad_sequences(pitch_seqs, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library importing for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word embedding\n",
    "embedding_vector_len = 7 #max pitch value length: 4, comma, significance value of length 1, newline character\n",
    "\n",
    "#Vocabulary size choice\n",
    "vocabulary_size = 58\n",
    "\n",
    "#Network topology\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocabulary_size, embedding_vector_len, input_length=max_seq_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(20, activation='sigmoid'))\n",
    "\n",
    "#Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
