{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional Neural Network for Makam Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library importing for file reading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.interpolate import interp2d \n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File reading and preprocessing\n",
    "\n",
    "Read all .mp3 files and retrieve their makam based on the folder that their equivalent .pitch files exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/librosa/core/audio.py:161: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 7396)\n",
      "1\n",
      "1\n",
      "[array([[ 2.83952995e-02-3.89740239e-05j,  1.10968372e-03-2.82432084e-02j,\n",
      "        -2.81479643e-02-1.54263462e-03j, ...,\n",
      "         0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
      "         0.00000000e+00+0.00000000e+00j],\n",
      "       [ 8.23263045e-03-3.99725673e-04j, -3.47274064e-03-9.69378004e-03j,\n",
      "        -1.25403658e-02+4.46751686e-03j, ...,\n",
      "         0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
      "         0.00000000e+00+0.00000000e+00j],\n",
      "       [-1.87201765e-02-1.32284282e-04j, -1.31531823e-02+1.36977796e-02j,\n",
      "         2.40908465e-04+1.90910697e-02j, ...,\n",
      "         0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
      "         0.00000000e+00+0.00000000e+00j],\n",
      "       ...,\n",
      "       [ 2.30075738e-06-2.02411164e-07j,  2.20067607e-04+5.59977871e-04j,\n",
      "        -8.98479333e-05+5.08891169e-04j, ...,\n",
      "         0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
      "         0.00000000e+00+0.00000000e+00j],\n",
      "       [-5.64719955e-06-4.34027847e-08j,  1.52639043e-04+3.88530174e-04j,\n",
      "         1.09251066e-04+5.90211966e-04j, ...,\n",
      "         0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
      "         0.00000000e+00+0.00000000e+00j],\n",
      "       [ 1.67001895e-06+2.99692060e-07j, -1.18057773e-05+3.74663937e-05j,\n",
      "         1.97753441e-04+3.59990134e-04j, ...,\n",
      "         0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n",
      "         0.00000000e+00+0.00000000e+00j]])]\n",
      "['Hicaz']\n"
     ]
    }
   ],
   "source": [
    "#Array containing all constant-Q tranforms of the soundfiles\n",
    "cqts = []\n",
    "\n",
    "#Array containing all makam labels\n",
    "Y = []\n",
    "\n",
    "#Makam list for more efficient file searching during label retrieval\n",
    "makams = [\"Acemasiran\", \"Acemkurdi\", \"Bestenigar\", \"Beyati\", \"Hicaz\", \"Hicazkar\", \"Huseyni\", \"Huzzam\", \"Karcigar\", \"Kurdilihicazkar\", \"Mahur\", \"Muhayyer\", \"Neva\", \"Nihavent\", \"Rast\", \"Saba\", \"Segah\", \"Sultaniyegah\", \"Suzinak\", \"Ussak\"]\n",
    "\n",
    "#Traverse directory\n",
    "for root, dirs, files in os.walk('./soundfiles'):\n",
    "        for name in files:\n",
    "            \n",
    "            #----------------------Labels------------------------#\n",
    "            \n",
    "            #find under which folder the file is (for makam retrieval) and append label set\n",
    "            matched = 0\n",
    "            for makam in makams:\n",
    "                if (os.path.isfile(\"./otmm_makam_recognition_dataset/data/\" + makam + \"/\" + name[:-4] + \".pitch\") == True):\n",
    "                    Y.append(makam)\n",
    "                    matched = 1\n",
    "                    break\n",
    "                    \n",
    "            #if soundfile not in pitch data, ignore\n",
    "            if (matched == 0):\n",
    "                continue\n",
    "            \n",
    "            #----------------Constant-Q Transform----------------#\n",
    "            \n",
    "            #construct soundfile directory\n",
    "            filedir = os.path.join(root, name)\n",
    "            \n",
    "            #load soundfile\n",
    "            y, sr = librosa.core.load(filedir)\n",
    "            \n",
    "            #compute the constant-Q transform from audio signal\n",
    "            cqt = librosa.core.cqt(y, sr, n_bins=371, bins_per_octave=53)\n",
    "            \n",
    "            #append constat-Q transform to set\n",
    "            cqts.append(cqt)\n",
    "            \n",
    "            print(cqt.shape)\n",
    "            \n",
    "print(len(cqts))\n",
    "print(len(Y))\n",
    "print(cqts)\n",
    "print(Y)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant-Q transform resampling\n",
    "CQTs are going to vary in length based on the length of the audio file. One wait to deal with this is resampling the CQTs to the common size (371, 3710). \n",
    "\n",
    "** Calculate average song length to justify that 3710 is a reasonable middle ground because some will need upscalling while most downscaling **\n",
    "\n",
    "** I don't think resampled arrays contain complex numbers anymore. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute all interpolation functions\n",
    "interpol_f = []\n",
    "for cqt in cqts:\n",
    "    Xindex = np.linspace(0, 1, num=371)\n",
    "    Yindex = np.linspace(0, 1, num=cqt.shape[1])\n",
    "    f = interp2d(Xindex, Yindex, cqt.flatten(), kind='linear')\n",
    "    interpol_f.append(f)\n",
    "    \n",
    "#Resample cqts\n",
    "X = []\n",
    "for i in range(len(cqts)):\n",
    "    Xindex_rs = np.linspace(0, 1, num=3710)\n",
    "    Yindex_rs = np.linspace(0, 1, num=371)\n",
    "    X.append(np.reshape(interpol_f[i](Xindex_rs, Yindex_rs), (371, 3710)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant-Q transform truncation\n",
    "An alternative to resampling the cqts would be to truncate all soundfiles to the length of the shortest soundfiles.\n",
    "\n",
    "** This seems like a less productive approach, but probably depends on duration deviation among soundfiles. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library importing for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network topology\n",
    "model = Sequential()\n",
    "\n",
    "#3 convolutional layers\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', input_shape=(371, 3710)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', input_shape=(371, 3710)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(2,2), activation='relu', input_shape=(371, 3710)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "#Output layer\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "#Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18750 samples, validate on 6250 samples\n",
      "Epoch 1/20\n",
      "18750/18750 [==============================] - 22s 1ms/step - loss: 0.6366 - acc: 0.6303 - val_loss: 0.6210 - val_acc: 0.6573\n",
      "Epoch 2/20\n",
      "18750/18750 [==============================] - 21s 1ms/step - loss: 0.5555 - acc: 0.7187 - val_loss: 0.5210 - val_acc: 0.7477\n",
      "Epoch 3/20\n",
      "18750/18750 [==============================] - 23s 1ms/step - loss: 0.5156 - acc: 0.7466 - val_loss: 0.5143 - val_acc: 0.7461\n",
      "Epoch 4/20\n",
      "18750/18750 [==============================] - 23s 1ms/step - loss: 0.4851 - acc: 0.7654 - val_loss: 0.4868 - val_acc: 0.7690\n",
      "Epoch 5/20\n",
      "18750/18750 [==============================] - 23s 1ms/step - loss: 0.4553 - acc: 0.7827 - val_loss: 0.4857 - val_acc: 0.7643\n",
      "Epoch 6/20\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.4290 - acc: 0.8021 - val_loss: 0.4967 - val_acc: 0.7658\n",
      "Epoch 7/20\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.3990 - acc: 0.8178 - val_loss: 0.5046 - val_acc: 0.7642\n",
      "Epoch 8/20\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.3697 - acc: 0.8334 - val_loss: 0.4770 - val_acc: 0.7787\n",
      "Epoch 9/20\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.3439 - acc: 0.8478 - val_loss: 0.4811 - val_acc: 0.7810\n",
      "Epoch 10/20\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.3131 - acc: 0.8645 - val_loss: 0.4934 - val_acc: 0.7851\n",
      "Epoch 11/20\n",
      "18750/18750 [==============================] - 24s 1ms/step - loss: 0.2847 - acc: 0.8778 - val_loss: 0.5435 - val_acc: 0.7810\n",
      "Epoch 12/20\n",
      "18750/18750 [==============================] - 24s 1ms/step - loss: 0.2538 - acc: 0.8947 - val_loss: 0.5792 - val_acc: 0.7704\n",
      "Epoch 13/20\n",
      "18750/18750 [==============================] - 24s 1ms/step - loss: 0.2252 - acc: 0.9086 - val_loss: 0.6212 - val_acc: 0.7645\n",
      "Epoch 14/20\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.1957 - acc: 0.9214 - val_loss: 0.5976 - val_acc: 0.7696\n",
      "Epoch 15/20\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.1710 - acc: 0.9322 - val_loss: 0.6502 - val_acc: 0.7746\n",
      "Epoch 16/20\n",
      "18750/18750 [==============================] - 24s 1ms/step - loss: 0.1488 - acc: 0.9419 - val_loss: 0.7828 - val_acc: 0.7765\n",
      "Epoch 17/20\n",
      "18750/18750 [==============================] - 24s 1ms/step - loss: 0.1279 - acc: 0.9501 - val_loss: 0.7764 - val_acc: 0.7752\n",
      "Epoch 18/20\n",
      "18750/18750 [==============================] - 24s 1ms/step - loss: 0.1082 - acc: 0.9591 - val_loss: 1.1102 - val_acc: 0.7517\n",
      "Epoch 19/20\n",
      "18750/18750 [==============================] - 24s 1ms/step - loss: 0.0936 - acc: 0.9660 - val_loss: 0.8660 - val_acc: 0.7670\n",
      "Epoch 20/20\n",
      "18750/18750 [==============================] - 24s 1ms/step - loss: 0.0789 - acc: 0.9718 - val_loss: 0.9465 - val_acc: 0.7702\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=0.25, epochs=20, batch_size=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
