{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makam pitch sequence classification with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library importing for file reading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pitch files\n",
    "\n",
    "Before proceeding, the pitch files on the CompMusic Dunya makam corpus need to be converted in the quantized pitch series encoding constructed as described in the pseudocode below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for (all lines):\n",
    "--->delete value if 0\n",
    "--->quantize value to scale (e.g. 53ET) // pad start of pitch value with zeros appropriately so that the length of the pitch value is 5 e.g. 130 -> 00130\n",
    "    \n",
    "for (all lines):\n",
    "--->merge all neighboring values to one, and keep track of the number of consecutive occurrences k e.g. 1300\\n 1200\\n 1200\\n 1200\\n 760\\n 760\\n will become 1300,1\\n 1200,3\\n 760,2\\n\n",
    "    \n",
    "find kmin and kmax\t\n",
    "\n",
    "for (all lines):\n",
    "--->if k < (kmin + 0.25*(kmax-kmin)): replace with 1\n",
    "--->if (kmin + 0.25*(kmax-kmin)) < k < (kmin + 0.75*(kmax-kmin)): replace with 2\n",
    "--->If k > (kmin + 0.75*(kmax-kmin)): replace with 3\n",
    "\n",
    "//We now have a quantized pitch series encoding with a line format of pitch,significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File reading, octave folded encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "File number: 1000\n[4, 8, 19, 13, 14, 0, 12, 17, 18, 13, 13, 4, 0, 3, 17, 2, 14, 4, 13, 10, 6, 19, 15, 8, 14, 9, 11, 1, 5, 5, 4, 2, 14, 18, 4, 10, 16, 11, 15, 5, 17, 10, 6, 9, 12, 3, 1, 14, 14, 14, 6, 5, 12, 17, 15, 17, 17, 14, 1, 14, 14, 16, 11, 7, 11, 5, 4, 2, 18, 6, 15, 7, 7, 8, 14, 1, 2, 13, 9, 3, 9, 16, 1, 8, 12, 13, 11, 13, 9, 9, 18, 14, 14, 1, 16, 18, 6, 1, 0, 6, 0, 9, 18, 14, 3, 14, 12, 3, 3, 4, 15, 9, 3, 18, 13, 14, 7, 5, 14, 19, 18, 10, 14, 15, 18, 3, 15, 3, 16, 8, 19, 15, 5, 5, 9, 1, 16, 7, 2, 5, 0, 5, 11, 10, 17, 10, 4, 0, 11, 15, 19, 17, 9, 14, 13, 0, 12, 15, 6, 0, 7, 16, 16, 16, 5, 1, 3, 7, 4, 12, 11, 10, 13, 11, 8, 12, 16, 17, 13, 15, 5, 14, 6, 2, 14, 14, 14, 11, 7, 19, 5, 15, 8, 11, 10, 10, 14, 17, 17, 0, 4, 2, 15, 19, 6, 17, 2, 7, 19, 8, 2, 9, 14, 6, 5, 15, 2, 6, 11, 15, 6, 5, 17, 16, 6, 6, 7, 19, 19, 10, 9, 4, 9, 14, 2, 10, 5, 19, 16, 14, 2, 9, 9, 11, 7, 1, 8, 9, 0, 12, 8, 8, 9, 0, 13, 11, 13, 5, 8, 1, 0, 8, 2, 17, 7, 0, 15, 11, 13, 2, 4, 13, 16, 11, 9, 14, 11, 17, 8, 1, 16, 13, 15, 18, 5, 13, 8, 6, 18, 10, 13, 10, 1, 18, 4, 8, 3, 7, 9, 5, 4, 19, 18, 11, 15, 7, 12, 1, 1, 0, 18, 9, 4, 0, 4, 17, 10, 6, 4, 12, 7, 18, 3, 12, 2, 8, 17, 13, 0, 13, 8, 1, 2, 9, 4, 7, 3, 14, 2, 3, 18, 11, 7, 3, 8, 5, 3, 5, 4, 16, 16, 3, 12, 8, 4, 6, 5, 17, 7, 5, 0, 18, 5, 19, 7, 3, 19, 19, 9, 12, 5, 16, 8, 4, 19, 3, 17, 5, 18, 12, 8, 8, 12, 19, 6, 19, 11, 13, 4, 2, 19, 11, 18, 12, 8, 3, 1, 3, 16, 3, 5, 16, 18, 12, 11, 13, 7, 4, 18, 10, 11, 7, 7, 8, 10, 3, 19, 17, 13, 15, 14, 0, 11, 4, 7, 2, 10, 16, 3, 6, 10, 8, 15, 9, 4, 2, 12, 3, 13, 15, 4, 10, 16, 19, 15, 13, 1, 16, 2, 3, 8, 10, 12, 6, 5, 1, 4, 18, 19, 17, 3, 16, 16, 18, 0, 14, 8, 6, 11, 6, 14, 8, 15, 18, 12, 6, 18, 16, 14, 3, 19, 4, 0, 13, 12, 17, 18, 15, 8, 6, 16, 0, 11, 8, 2, 16, 0, 11, 11, 19, 1, 6, 6, 4, 7, 5, 16, 17, 17, 8, 2, 8, 17, 11, 19, 5, 15, 4, 19, 19, 7, 3, 11, 10, 14, 16, 12, 14, 1, 6, 6, 4, 1, 12, 18, 2, 12, 4, 6, 3, 16, 16, 17, 0, 3, 0, 17, 0, 2, 8, 19, 2, 3, 2, 7, 4, 9, 6, 5, 19, 19, 19, 2, 7, 18, 2, 18, 2, 0, 2, 8, 6, 3, 11, 19, 14, 9, 15, 14, 18, 19, 7, 1, 10, 19, 17, 1, 8, 4, 6, 19, 12, 3, 14, 15, 1, 16, 12, 14, 18, 15, 9, 7, 8, 10, 10, 4, 4, 1, 11, 19, 9, 16, 7, 15, 3, 7, 12, 9, 5, 19, 0, 10, 15, 8, 4, 6, 18, 5, 2, 3, 18, 5, 6, 15, 17, 12, 9, 12, 16, 6, 9, 17, 18, 16, 12, 13, 15, 12, 0, 16, 9, 1, 8, 18, 3, 12, 5, 7, 1, 13, 10, 8, 14, 17, 12, 9, 17, 10, 12, 17, 15, 7, 8, 5, 2, 18, 0, 10, 8, 13, 17, 14, 19, 9, 0, 17, 17, 10, 18, 15, 13, 6, 12, 17, 14, 14, 12, 3, 12, 19, 10, 0, 1, 7, 10, 10, 1, 13, 18, 18, 10, 6, 2, 2, 0, 9, 15, 16, 10, 0, 11, 17, 0, 8, 1, 18, 3, 11, 8, 9, 5, 9, 9, 17, 18, 9, 11, 2, 17, 2, 14, 12, 2, 0, 6, 0, 9, 10, 19, 12, 15, 7, 13, 0, 5, 4, 5, 8, 16, 13, 11, 4, 5, 10, 14, 15, 5, 11, 4, 2, 7, 12, 11, 4, 2, 7, 17, 0, 9, 6, 9, 10, 6, 10, 18, 17, 0, 1, 2, 4, 13, 19, 16, 9, 8, 18, 1, 9, 0, 6, 16, 12, 14, 18, 2, 18, 10, 18, 3, 15, 10, 7, 13, 11, 5, 19, 8, 3, 11, 1, 4, 16, 15, 6, 1, 1, 1, 5, 15, 14, 3, 19, 12, 7, 7, 7, 6, 5, 11, 10, 5, 10, 17, 2, 0, 13, 15, 14, 1, 3, 11, 11, 13, 2, 11, 16, 13, 3, 10, 15, 1, 1, 3, 18, 15, 1, 4, 17, 15, 0, 7, 1, 5, 6, 15, 17, 7, 14, 13, 3, 7, 5, 18, 16, 12, 16, 11, 17, 15, 10, 13, 1, 7, 19, 15, 1, 1, 6, 12, 8, 6, 13, 7, 13, 19, 2, 16, 19, 19, 9, 0, 10, 4, 2, 13, 19, 2, 7, 14, 9, 9, 7, 1, 0, 16, 5, 11, 12, 17, 0, 0, 13, 2, 11, 12, 5, 6, 9, 18, 8, 3, 10, 10, 19, 13, 3, 17, 6, 4, 1, 12, 0, 9, 10, 1, 13, 6, 11, 12, 10, 13, 18, 9, 8, 13, 17, 5, 19, 3, 2, 15, 16, 13, 4, 0, 15, 7, 0, 12, 11, 16, 9, 17, 6, 0, 4, 16, 1, 3, 15, 4, 10, 7, 13, 16, 1, 4, 11, 2]\nArray length: 1000\n"
    }
   ],
   "source": [
    "#quantized octave-folded pitch file directory\n",
    "ofq_read_dir = \"./octfold_qdata/\" \n",
    "\n",
    "#Makam list for more efficient file searching during label retrieval\n",
    "makams = [\"Acemasiran\", \"Acemkurdi\", \"Bestenigar\", \"Beyati\", \"Hicaz\", \"Hicazkar\", \"Huseyni\", \"Huzzam\", \"Karcigar\", \"Kurdilihicazkar\", \"Mahur\", \"Muhayyer\", \"Neva\", \"Nihavent\", \"Rast\", \"Saba\", \"Segah\", \"Sultaniyegah\", \"Suzinak\", \"Ussak\"]\n",
    "\n",
    "all_qpitch = [] #array holding all quantized pitch series as strings\n",
    "y = [] #holds makam labels\n",
    "max_length = 0\n",
    "for root, dirs, files in os.walk(ofq_read_dir):\n",
    "    for name in files:\n",
    "        if '.pitch' in name:\n",
    "            #retrieve label from parent of original path\n",
    "            for makam in makams:\n",
    "                if (os.path.isfile(\"./otmm_makam_recognition_dataset/data/\" + makam + \"/\" + name) == True):\n",
    "                    y.append(makams.index(makam))\n",
    "                    break\n",
    "            \n",
    "            with open(os.path.join(root, name)) as f:\n",
    "                content = f.read()\n",
    "                all_qpitch.append(content)\n",
    "print(\"File number:\", len(y))\n",
    "print(\"Array length:\", len(all_qpitch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Pading input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "top_k = 636 #53*4*3\n",
    "tokenizer = Tokenizer(num_words=top_k, split='\\n')\n",
    "tokenizer.fit_on_texts(all_qpitch)\n",
    "seqs = tokenizer.texts_to_sequences(all_qpitch)\n",
    "print(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[15  5  2 ...  0  0  0]\n [11  7  2 ...  0  0  0]\n [47  4  1 ...  0  0  0]\n ...\n [12  8  1 ...  0  0  0]\n [14  7  1 ...  0  0  0]\n [33  4  1 ...  0  0  0]]\n"
    }
   ],
   "source": [
    "#Padding\n",
    "X = sequence.pad_sequences(seqs, padding='post')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library importing for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word embedding\n",
    "embedding_vector_len = 7 #max pitch value length: 4, comma, significance value of length 1, newline \n",
    "\n",
    "#Network topology\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(top_k, embedding_vector_len, input_length=len(X[0])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_6\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_6 (Embedding)      (None, 213393, 7)         4452      \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 213393, 7)         0         \n_________________________________________________________________\nlstm_5 (LSTM)                (None, 100)               43200     \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 100)               0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 101       \n=================================================================\nTotal params: 47,753\nTrainable params: 47,753\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "#Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\n"
    }
   ],
   "source": [
    "#Fitting\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model evaluation\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}