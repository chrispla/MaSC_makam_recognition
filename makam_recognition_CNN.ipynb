{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional Neural Network for Makam Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library importing for file reading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from scipy.interpolate import interp2d \n",
    "from sklearn.preprocessing import normalize\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File reading and preprocessing\n",
    "\n",
    "Read all .mp3 files and retrieve their makam based on the folder that their equivalent .pitch files exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f74129d5846b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m#load soundfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiledir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m#compute the constant-Q transform from audio signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr_native\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(y, orig_sr, target_sr, res_type, fix, scale, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sr\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mgcd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_sr\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mgcd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresampy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/resampy/core.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(x, sr_orig, sr_new, axis, filter, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mx_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0my_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mresample_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp_win\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp_delta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Array containing all constant-Q tranforms of the soundfiles\n",
    "cqts = []\n",
    "\n",
    "#Array containing all makam labels\n",
    "Y = []\n",
    "\n",
    "#Makam list for more efficient file searching during label retrieval\n",
    "makams = [\"Acemasiran\", \"Acemkurdi\", \"Bestenigar\", \"Beyati\", \"Hicaz\", \"Hicazkar\", \"Huseyni\", \"Huzzam\", \"Karcigar\", \"Kurdilihicazkar\", \"Mahur\", \"Muhayyer\", \"Neva\", \"Nihavent\", \"Rast\", \"Saba\", \"Segah\", \"Sultaniyegah\", \"Suzinak\", \"Ussak\"]\n",
    "\n",
    "#Ignore mp3 read warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"PySoundFile failed. Trying audioread instead.\")\n",
    "\n",
    "index = 0\n",
    "#Traverse directory\n",
    "for root, dirs, files in os.walk('./soundfiles'):\n",
    "        for name in files:\n",
    "            \n",
    "            #----------------------Labels------------------------#\n",
    "            \n",
    "            #find under which folder the file is (for makam retrieval) and append label set\n",
    "            matched = 0\n",
    "            for makam in makams:\n",
    "                if (os.path.isfile(\"./otmm_makam_recognition_dataset/data/\" + makam + \"/\" + name[:-4] + \".pitch\") == True):\n",
    "                    Y.append(makams.index(makam))\n",
    "                    matched = 1\n",
    "                    break\n",
    "                    \n",
    "            #if soundfile not in pitch data, ignore\n",
    "            if (matched == 0):\n",
    "                continue\n",
    "            \n",
    "            #----------------Constant-Q Transform----------------#\n",
    "            \n",
    "            #construct soundfile directory\n",
    "            filedir = os.path.join(root, name)\n",
    "            \n",
    "            #load soundfile\n",
    "            y, sr = librosa.core.load(filedir)\n",
    "            \n",
    "            #compute the constant-Q transform from audio signal\n",
    "            cqt = librosa.core.cqt(y, sr, n_bins=371, bins_per_octave=53)\n",
    "            \n",
    "            #Resampling\n",
    "            interpol_f = []\n",
    "            for cqt in cqts:\n",
    "                Xindex = np.linspace(0, 1, num=371)\n",
    "                Yindex = np.linspace(0, 1, num=cqt.shape[1])\n",
    "                f = interp2d(Xindex, Yindex, cqt.flatten(), kind='linear')\n",
    "    \n",
    "            #Resample cqts\n",
    "            X = np.zeros((371, 1855), dtype=complex)\n",
    "            for i in range(len(cqts)):\n",
    "                Xindex_rs = np.linspace(0, 1, num=371)\n",
    "                Yindex_rs = np.linspace(0, 1, num=3710)\n",
    "                X.append(np.reshape(interpol_f[i](Xindex_rs, Yindex_rs), (371, 3710)))\n",
    "            \n",
    "            #append constat-Q transform to set\n",
    "            cqts.append(cqt)\n",
    "            \n",
    "            #print files processed\n",
    "            print(index, end=\", \")\n",
    "            index+=1\n",
    "\n",
    "            \n",
    "print(len(cqts))\n",
    "print(len(Y))\n",
    "print(cqts)\n",
    "print(Y)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant-Q transform resampling\n",
    "CQTs are going to vary in length based on the length of the audio file. One wait to deal with this is resampling the CQTs to the common size (371, 3710). \n",
    "\n",
    "** Calculate average song length to justify that 3710 is a reasonable middle ground because some will need upscalling while most downscaling **\n",
    "\n",
    "** I don't think resampled arrays contain complex numbers anymore. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/interpolate/interpolate.py:243: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  kx=kx, ky=ky, s=0.0)\n"
     ]
    }
   ],
   "source": [
    "#Compute all interpolation functions\n",
    "interpol_f = []\n",
    "for cqt in cqts:\n",
    "    Xindex = np.linspace(0, 1, num=371)\n",
    "    Yindex = np.linspace(0, 1, num=cqt.shape[1])\n",
    "    f = interp2d(Xindex, Yindex, cqt.flatten(), kind='linear')\n",
    "    interpol_f.append(f)\n",
    "    \n",
    "#Resample cqts\n",
    "X = np.zeros((371, 3710), dtype=complex)\n",
    "for i in range(len(cqts)):\n",
    "    Xindex_rs = np.linspace(0, 1, num=371)\n",
    "    Yindex_rs = np.linspace(0, 1, num=3710)\n",
    "    X.append(np.reshape(interpol_f[i](Xindex_rs, Yindex_rs), (371, 3710)))\n",
    "    \n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant-Q transform truncation\n",
    "An alternative to resampling the cqts would be to truncate all soundfiles to the length of the shortest soundfiles.\n",
    "\n",
    "** This seems like a less productive approach, but probably depends on duration deviation among soundfiles. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library importing for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_c = to_categorical(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_c, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network topology\n",
    "model = Sequential()\n",
    "\n",
    "#3 convolutional layers\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', input_shape=(371, 3710)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', input_shape=(371, 3710)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(2,2), activation='relu', input_shape=(371, 3710)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "\n",
    "#Output layer\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "#Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18750 samples, validate on 6250 samples\n",
      "Epoch 1/20\n",
      "18750/18750 [==============================] - 22s 1ms/step - loss: 0.6366 - acc: 0.6303 - val_loss: 0.6210 - val_acc: 0.6573\n",
      "Epoch 2/20\n",
      "18750/18750 [==============================] - 21s 1ms/step - loss: 0.5555 - acc: 0.7187 - val_loss: 0.5210 - val_acc: 0.7477\n",
      "Epoch 3/20\n",
      "18750/18750 [==============================] - 23s 1ms/step - loss: 0.5156 - acc: 0.7466 - val_loss: 0.5143 - val_acc: 0.7461\n",
      "Epoch 4/20\n",
      "18750/18750 [==============================] - 23s 1ms/step - loss: 0.4851 - acc: 0.7654 - val_loss: 0.4868 - val_acc: 0.7690\n",
      "Epoch 5/20\n",
      "18750/18750 [==============================] - 23s 1ms/step - loss: 0.4553 - acc: 0.7827 - val_loss: 0.4857 - val_acc: 0.7643\n",
      "Epoch 6/20\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.4290 - acc: 0.8021 - val_loss: 0.4967 - val_acc: 0.7658\n",
      "Epoch 7/20\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.3990 - acc: 0.8178 - val_loss: 0.5046 - val_acc: 0.7642\n",
      "Epoch 8/20\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.3697 - acc: 0.8334 - val_loss: 0.4770 - val_acc: 0.7787\n",
      "Epoch 9/20\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.3439 - acc: 0.8478 - val_loss: 0.4811 - val_acc: 0.7810\n",
      "Epoch 10/20\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.3131 - acc: 0.8645 - val_loss: 0.4934 - val_acc: 0.7851\n",
      "Epoch 11/20\n",
      "18750/18750 [==============================] - 24s 1ms/step - loss: 0.2847 - acc: 0.8778 - val_loss: 0.5435 - val_acc: 0.7810\n",
      "Epoch 12/20\n",
      "18750/18750 [==============================] - 24s 1ms/step - loss: 0.2538 - acc: 0.8947 - val_loss: 0.5792 - val_acc: 0.7704\n",
      "Epoch 13/20\n",
      "18750/18750 [==============================] - 24s 1ms/step - loss: 0.2252 - acc: 0.9086 - val_loss: 0.6212 - val_acc: 0.7645\n",
      "Epoch 14/20\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.1957 - acc: 0.9214 - val_loss: 0.5976 - val_acc: 0.7696\n",
      "Epoch 15/20\n",
      "18750/18750 [==============================] - 25s 1ms/step - loss: 0.1710 - acc: 0.9322 - val_loss: 0.6502 - val_acc: 0.7746\n",
      "Epoch 16/20\n",
      "18750/18750 [==============================] - 24s 1ms/step - loss: 0.1488 - acc: 0.9419 - val_loss: 0.7828 - val_acc: 0.7765\n",
      "Epoch 17/20\n",
      "18750/18750 [==============================] - 24s 1ms/step - loss: 0.1279 - acc: 0.9501 - val_loss: 0.7764 - val_acc: 0.7752\n",
      "Epoch 18/20\n",
      "18750/18750 [==============================] - 24s 1ms/step - loss: 0.1082 - acc: 0.9591 - val_loss: 1.1102 - val_acc: 0.7517\n",
      "Epoch 19/20\n",
      "18750/18750 [==============================] - 24s 1ms/step - loss: 0.0936 - acc: 0.9660 - val_loss: 0.8660 - val_acc: 0.7670\n",
      "Epoch 20/20\n",
      "18750/18750 [==============================] - 24s 1ms/step - loss: 0.0789 - acc: 0.9718 - val_loss: 0.9465 - val_acc: 0.7702\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=0.25, epochs=20, batch_size=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
