{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional Neural Network for Makam Recognition\n",
    "10-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library importing for file reading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from mutagen.mp3 import MP3\n",
    "import librosa\n",
    "from scipy.interpolate import interp2d \n",
    "from sklearn.preprocessing import normalize\n",
    "import warnings\n",
    "from math import floor\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session('cnn10fold.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.load_session('cnn10fold.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File reading using provided folds and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fold 0.\n",
      "899/100Loading fold 1.\n",
      "899/100Fold loading completed.\n",
      "X_train shape: 10 900 (371, 646)\n",
      "X_test shape: 10 100 (371, 646)\n",
      "y_train shape: 10\n",
      "y_test shape: 10\n"
     ]
    }
   ],
   "source": [
    "#--------PARAMETERS--------#\n",
    "\n",
    "#Choose truncation option\n",
    "# 1 : 15 seconds starting from the start\n",
    "# 2 : 15 seconds around the middle\n",
    "# 3 : 15 seconds from the end\n",
    "trunc_option = 2\n",
    "\n",
    "#Choose bins and bins per octave\n",
    "n_bins = 371\n",
    "bins_per_octave = 53\n",
    "\n",
    "#--------------------------#\n",
    "\n",
    "#Ignore mp3 read warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"PySoundFile failed. Trying audioread instead.\")\n",
    "\n",
    "makams = [\"Acemasiran\", \"Acemkurdi\", \"Bestenigar\", \"Beyati\", \"Hicaz\", \"Hicazkar\", \"Huseyni\", \"Huzzam\", \"Karcigar\", \"Kurdilihicazkar\", \"Mahur\", \"Muhayyer\", \"Neva\", \"Nihavent\", \"Rast\", \"Saba\", \"Segah\", \"Sultaniyegah\", \"Suzinak\", \"Ussak\"]\n",
    "\n",
    "#Arrays containing all constant-Q transforms of the soundfiles per fold\n",
    "X_train = [[],[],[],[],[],[],[],[],[],[]]\n",
    "X_test = [[],[],[],[],[],[],[],[],[],[]]\n",
    "\n",
    "#Array containing all makam labels per fold\n",
    "y_train = [[],[],[],[],[],[],[],[],[],[]]\n",
    "y_test = [[],[],[],[],[],[],[],[],[],[]]\n",
    "\n",
    "#Import folds\n",
    "f = open(\"./dlfm_makam_recognition_data/folds_updated.json\")\n",
    "folds = json.load(f)\n",
    "\n",
    "\n",
    "#Compute cqts and import labels\n",
    "for i in range(len(folds)): #in number of folds\n",
    "    \n",
    "    print(\"\\nLoading fold \" + str(i) + \".\")\n",
    "    \n",
    "    #compute all training cqts and import train labels\n",
    "    for source in range(len(folds[i][1][\"training\"][\"sources\"])):\n",
    "        \n",
    "        file_name = (\"./soundfiles/\" + folds[i][1][\"training\"][\"sources\"][source] + \".mp3\")\n",
    "\n",
    "        #get file duration for loading\n",
    "        try:\n",
    "            audio = MP3(file_name)\n",
    "        except:\n",
    "            print(\"Problem with:\", file_name)\n",
    "            continue\n",
    "        audio = MP3(file_name)\n",
    "        duration = audio.info.length\n",
    "        \n",
    "        #load 15 seconds from the mp3 according to truncation option\n",
    "        if (trunc_option == 1): #15 seconds form the start\n",
    "            #load soundfile\n",
    "            y, sr = librosa.core.load(file_name, duration=15)\n",
    "            #compute the constant-Q transform from audio signal\n",
    "            cqt = librosa.core.cqt(y, sr, n_bins=n_bins, bins_per_octave=bins_per_octave) #up to C7 in 53TET\n",
    "            \n",
    "        if (trunc_option == 2): #15 seconds around the center\n",
    "            offset = (duration-15)/2\n",
    "            #load soundfile\n",
    "            y, sr = librosa.core.load(file_name, offset=offset, duration=15)\n",
    "            #compute the constant-Q transform from audio signal\n",
    "            cqt = librosa.core.cqt(y, sr, n_bins=n_bins, bins_per_octave=bins_per_octave) #up to C7 in 53TET\n",
    "            \n",
    "        if (trunc_option == 3): #15 seconds from the end\n",
    "            offset = duration-15\n",
    "            #load soundfile\n",
    "            y, sr = librosa.core.load(file_name, offset=offset, duration=15)\n",
    "            #compute the constant-Q transform from audio signal\n",
    "            cqt = librosa.core.cqt(y, sr, n_bins=n_bins, bins_per_octave=bins_per_octave) #up to C7 in 53TET\n",
    "        \n",
    "        #Normalization\n",
    "        cqt = (cqt - np.mean(cqt)) / (np.std(cqt) + 1e-8)\n",
    "\n",
    "        X_train[i].append(cqt)\n",
    "        \n",
    "        #import makam training label\n",
    "        y_train[i].append(makams.index(folds[i][1][\"training\"][\"modes\"][source]))\n",
    "        \n",
    "        sys.stdout.write(\"\\rLoading training %i/900\" % source)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        \n",
    "    #compute all testing cqts and import test labels\n",
    "    for ref in range(len(folds[i][1][\"testing\"])):\n",
    "        \n",
    "        file_name = (\"./soundfiles/\" + folds[i][1][\"testing\"][ref][\"source\"] + \".mp3\")\n",
    "\n",
    "        #get file duration for loading\n",
    "        try:\n",
    "            audio = MP3(file_name)\n",
    "        except:\n",
    "            print(\"Problem with:\", file_name)\n",
    "            continue\n",
    "        duration = audio.info.length\n",
    "        \n",
    "        #load 15 seconds from the mp3 according to truncation option\n",
    "        if (trunc_option == 1): #15 seconds form the start\n",
    "            #load soundfile\n",
    "            y, sr = librosa.core.load(file_name, duration=15)\n",
    "            #compute the constant-Q transform from audio signal\n",
    "            cqt = librosa.core.cqt(y, sr, n_bins=n_bins, bins_per_octave=bins_per_octave) #up to C7 in 53TET\n",
    "            \n",
    "        if (trunc_option == 2): #15 seconds around the center\n",
    "            offset = (duration-15)/2\n",
    "            #load soundfile\n",
    "            y, sr = librosa.core.load(file_name, offset=offset, duration=15)\n",
    "            #compute the constant-Q transform from audio signal\n",
    "            cqt = librosa.core.cqt(y, sr, n_bins=n_bins, bins_per_octave=bins_per_octave) #up to C7 in 53TET\n",
    "            \n",
    "        if (trunc_option == 3): #15 seconds from the end\n",
    "            offset = duration-15\n",
    "            #load soundfile\n",
    "            y, sr = librosa.core.load(file_name, offset=offset, duration=15)\n",
    "            #compute the constant-Q transform from audio signal\n",
    "            cqt = librosa.core.cqt(y, sr, n_bins=n_bins, bins_per_octave=bins_per_octave) #up to C7 in 53TET\n",
    "        \n",
    "        #Normalization\n",
    "        cqt = (cqt - np.mean(cqt)) / (np.std(cqt) + 1e-8)\n",
    "        \n",
    "        X_test[i].append(cqt)\n",
    "        \n",
    "        #import makam testing label\n",
    "        y_test[i].append(makams.index(folds[i][1][\"testing\"][ref][\"mode\"]))\n",
    "        \n",
    "        sys.stdout.write(\"\\rLoading testing %i/100\" % ref)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "print(\"Fold loading completed.\")\n",
    "\n",
    "print(\"X_train shape:\", len(X_train), len(X_train[0]), X_train[0][0].shape)\n",
    "print(\"X_test shape:\", len(X_test), len(X_test[0]), X_test[0][0].shape)\n",
    "print(\"y_train shape:\", len(y_train))\n",
    "print(\"y_test shape:\", len(y_test))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library importing for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, BatchNormalization, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (900, 371, 646, 1)\n",
      "X_test shape: (100, 371, 646, 1)\n",
      "y_train shape: (900, 20)\n",
      "y_test shape: (100, 20)\n"
     ]
    }
   ],
   "source": [
    "#one-hot encode target vector\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "for i in range(len(folds)):\n",
    "    Y_train.append(to_categorical(np.asarray(y_train[i])))\n",
    "    Y_test.append(to_categorical(np.asarray(y_test[i])))\n",
    "#channel dimension\n",
    "for i in range(len(folds)):\n",
    "    X_train[i] = (np.asarray(X_train[i])).reshape(len(X_train[i]), X_train[i][0].shape[0], X_train[i][0].shape[1], 1)\n",
    "    X_test[i] = (np.asarray(X_test[i])).reshape(len(X_test[i]), X_test[i][0].shape[0], X_test[i][0].shape[1], 1)\n",
    "print(\"X_train shape:\", X_train[0].shape)\n",
    "print(\"X_test shape:\", X_test[0].shape)\n",
    "print(\"y_train shape:\", Y_train[0].shape)\n",
    "print(\"y_test shape:\", Y_test[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 369, 644, 64)      640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 184, 322, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 184, 322, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 182, 320, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 91, 160, 64)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 91, 160, 64)       256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 90, 159, 64)       16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 45, 79, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 45, 79, 64)        256       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 227520)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                14561344  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                1300      \n",
      "=================================================================\n",
      "Total params: 14,617,428\n",
      "Trainable params: 14,617,044\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 369, 644, 64)      640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 184, 322, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 184, 322, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 182, 320, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 91, 160, 64)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 91, 160, 64)       256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 90, 159, 64)       16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 45, 79, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 45, 79, 64)        256       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 227520)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                14561344  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                1300      \n",
      "=================================================================\n",
      "Total params: 14,617,428\n",
      "Trainable params: 14,617,044\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(len(folds)):\n",
    "    #Network topology\n",
    "    model = Sequential()\n",
    "\n",
    "    #3 convolutional layers\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', input_shape=(X_train[i][0].shape[0], X_train[i][0].shape[1], 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', input_shape=(X_train[i][0].shape[0], X_train[i][0].shape[1], 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(2,2), activation='relu', input_shape=(X_train[i][0].shape[0], X_train[i][0].shape[1], 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    #Output layer\n",
    "    model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "    #Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 675 samples, validate on 225 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py:96: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return ops.EagerTensor(value, ctx.device_name, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 245s 363ms/step - loss: 1.3738 - accuracy: 0.9035 - val_loss: 0.8469 - val_accuracy: 0.9122\n",
      "Epoch 2/10\n",
      "675/675 [==============================] - 233s 345ms/step - loss: 1.4614 - accuracy: 0.9021 - val_loss: 1.3819 - val_accuracy: 0.9000\n",
      "Epoch 3/10\n",
      "675/675 [==============================] - 218s 323ms/step - loss: 1.3254 - accuracy: 0.9064 - val_loss: 1.5142 - val_accuracy: 0.9000\n",
      "Epoch 4/10\n",
      "675/675 [==============================] - 221s 327ms/step - loss: 1.3997 - accuracy: 0.9064 - val_loss: 1.4850 - val_accuracy: 0.9000\n",
      "Epoch 5/10\n",
      "675/675 [==============================] - 222s 329ms/step - loss: 1.4231 - accuracy: 0.9055 - val_loss: 1.5337 - val_accuracy: 0.9000\n",
      "Epoch 6/10\n",
      "675/675 [==============================] - 256s 380ms/step - loss: 1.3517 - accuracy: 0.9079 - val_loss: 1.4355 - val_accuracy: 0.9000\n",
      "Epoch 7/10\n",
      "675/675 [==============================] - 295s 437ms/step - loss: 1.3582 - accuracy: 0.9075 - val_loss: 1.4378 - val_accuracy: 0.9000\n",
      "Epoch 8/10\n",
      "675/675 [==============================] - 275s 407ms/step - loss: 1.3807 - accuracy: 0.9076 - val_loss: 1.4085 - val_accuracy: 0.9000\n",
      "Epoch 9/10\n",
      "675/675 [==============================] - 255s 377ms/step - loss: 1.4190 - accuracy: 0.9061 - val_loss: 1.5336 - val_accuracy: 0.9000\n",
      "Epoch 10/10\n",
      "675/675 [==============================] - 207s 306ms/step - loss: 1.4220 - accuracy: 0.9065 - val_loss: 1.5093 - val_accuracy: 0.9009\n",
      "Train on 675 samples, validate on 225 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py:96: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return ops.EagerTensor(value, ctx.device_name, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675/675 [==============================] - 247s 366ms/step - loss: 1.3032 - accuracy: 0.9067 - val_loss: 0.9250 - val_accuracy: 0.9187\n",
      "Epoch 2/10\n",
      "675/675 [==============================] - 259s 384ms/step - loss: 1.3960 - accuracy: 0.9064 - val_loss: 1.1891 - val_accuracy: 0.9044\n",
      "Epoch 3/10\n",
      "675/675 [==============================] - 217s 322ms/step - loss: 1.4318 - accuracy: 0.9050 - val_loss: 1.2478 - val_accuracy: 0.9102\n",
      "Epoch 4/10\n",
      "675/675 [==============================] - 228s 338ms/step - loss: 1.5005 - accuracy: 0.9013 - val_loss: 1.2248 - val_accuracy: 0.9200\n",
      "Epoch 5/10\n",
      "675/675 [==============================] - 219s 324ms/step - loss: 1.5145 - accuracy: 0.9006 - val_loss: 1.2286 - val_accuracy: 0.9196\n",
      "Epoch 6/10\n",
      "675/675 [==============================] - 220s 325ms/step - loss: 1.4877 - accuracy: 0.9021 - val_loss: 1.2270 - val_accuracy: 0.9200\n",
      "Epoch 7/10\n",
      "675/675 [==============================] - 256s 380ms/step - loss: 1.3898 - accuracy: 0.9062 - val_loss: 1.2274 - val_accuracy: 0.9173\n",
      "Epoch 8/10\n",
      "675/675 [==============================] - 254s 376ms/step - loss: 1.4223 - accuracy: 0.9050 - val_loss: 1.2270 - val_accuracy: 0.9200\n",
      "Epoch 9/10\n",
      "675/675 [==============================] - 266s 393ms/step - loss: 1.4059 - accuracy: 0.9065 - val_loss: 1.5337 - val_accuracy: 0.9000\n",
      "Epoch 10/10\n",
      "675/675 [==============================] - 240s 356ms/step - loss: 1.4088 - accuracy: 0.9068 - val_loss: 1.5337 - val_accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "histories = []\n",
    "for i in range(len(folds)):\n",
    "    history = models[i].fit(X_train[i], Y_train[i], validation_split=0.25, epochs=5, batch_size=30)\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 6s 65ms/step\n",
      "100/100 [==============================] - 6s 64ms/step\n",
      "[[1.4257094478607177, 0.9049999117851257], [1.4570238018035888, 0.9049999117851257]]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "y_pred = []\n",
    "for i in range(len(folds)):\n",
    "    score = models[i].evaluate(X_test[i], Y_test[i], verbose=1)\n",
    "    scores.append(score)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
